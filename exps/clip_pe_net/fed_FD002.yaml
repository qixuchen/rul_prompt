# 0.001
# 1000 Test_RMSE_cor = 21.68059, Test_RULscore_min = 2270.66856
# 2000 Test_RMSE_cor = 21.95223, Test_RULscore_min = 2385.52120
# 3000 Test_RMSE_cor = 21.79434, Test_RULscore_min = 2256.48075
# 4000 Test_RMSE_cor = 20.32878, Test_RULscore_min = 2015.38694
# 5000 Test_RMSE_cor = 20.68524, Test_RULscore_min = 2028.47169

# 0.005
# 1000 Test_RMSE_cor = 19.64252, Test_RULscore_min = 2175.67079
# 2000 Test_RMSE_cor = 21.62989, Test_RULscore_min = 2401.23523
# 3000 Test_RMSE_cor = 21.48933, Test_RULscore_min = 2092.12168
# 4000 Test_RMSE_cor = 20.45824, Test_RULscore_min = 1957.70154
# 5000 Test_RMSE_cor = 20.33014, Test_RULscore_min = 1930.57290

# 0.01
# 3000 
# 4000 
# 5000 Test_RMSE_cor = 20.68524, Test_RULscore_min = 2028.47169

---
gpu: '0'
save_frequency: 5
seed: 5000
task: 't1'

dist_backend: 'nccl'
world_size: -1
rank: -1
dist_url: 'tcp://224.66.41.62:23456'
multiprocessing_distributed: False
distributed: False
#dataset
data:
  root: './data'
  set: 'FD002'
  max_rul: 125
  seq_len: 30 
  num_worker: 4

#network
net:
  name: 'penet_clip'
  hand_craft: False
  input_dim: 14
  aux_dim: 4
  num_hidden: 18
  hand_dim: 28

#fedrated learning
fed:
  n_user: 5
  n_user_per_iter: 2
  sample_interval: 1
  iid: 'iid'

#train
train:
  resume_epoch: 0
  batch_size: 128
  lr: 0.01
  optimizer: 'adam'
  lr_epoch: [100]
  lr_factor: 0.1
  end_epoch: 100
  callback_freq: 50
  warmup_iters: 0

# test
test:
  model_name: 'exp_lstm-0050.pth'
  model_path: './output/basic/FD002'
  test_freq: 1